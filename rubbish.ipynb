{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"rubbish.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMBYxVus1TaZl1C/nDbJZ9z"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"tFIy-5rxJ8Ai"},"outputs":[],"source":["from __future__ import print_function, division\n","\n","import glob\n","#import os\n","#import cv2\n","import scipy.io as sio\n","\n","import torchvision.transforms.functional as TF\n","from torchvision import transforms\n","\n","class MPI_Preprocess(nn.Module):\n","\n","    def __init__(self):\n","      super().__init__()\n","      self.norm = transforms.Normalize([0.5], [0.5])\n","\n","    @torch.no_grad()  # disable gradients for effiency\n","    def forward(self, x):\n","\n","        img = TF.to_tensor(x.copy()) # CxHxW\n","        #print(\"preproccess\")\n","        #print(torch.max(x_out))\n","        #print(torch.min(x_out))\n","        img = img.float()\n","\n","        x, y = (96, 160)\n","        x = int((256/224) * x)\n","        y = int((256/224) * y)\n","        output_size = (x, y)\n","        img = TF.resize(img, output_size)\n","\n","\n","        h, w = img.shape[-2:]\n","        new_h, new_w = (96, 160)\n","\n","        top =  int((h - new_h)/2)\n","        left =  int((w - new_w)/2)\n","\n","        img = img[:, top: top + new_h, left: left + new_w]\n","\n","        img = self.norm(img)\n","\n","        return img\n","\n","#\"./gazeset/imgs\"\n","\n","class MPIIGaze(Dataset):\n","\n","    def __init__(self, mpii_dir: str = './mpi/MPIIGaze'):\n","\n","        self.mpii_dir = mpii_dir\n","\n","        eval_files = glob.glob(f'{mpii_dir}/Evaluation Subset/sample list for eye image/*.txt')\n","\n","        self.trans = MPI_Preprocess()\n","\n","        self.eval_entries = []\n","        for ef in eval_files:\n","            person = os.path.splitext(os.path.basename(ef))[0]\n","            with open(ef) as f:\n","                lines = f.readlines()\n","                for line in lines:\n","                    line = line.strip()\n","                    if line != '':\n","                        img_path, side = [x.strip() for x in line.split()]\n","                        day, img = img_path.split('/')\n","                        self.eval_entries.append({\n","                            'day': day,\n","                            'img_name': img,\n","                            'person': person,\n","                            'side': side\n","                        })\n","\n","    def __len__(self):\n","        return len(self.eval_entries)\n","\n","    def __getitem__(self, idx):\n","        if torch.is_tensor(idx):\n","            idx = idx.tolist()\n","\n","        return self._load_sample(idx)\n","\n","    def _load_sample(self, i):\n","        entry = self.eval_entries[i]\n","        mat_path = os.path.join(self.mpii_dir, 'Data/Normalized', entry['person'], entry['day'] + '.mat')\n","        mat = sio.loadmat(mat_path)\n","\n","        filenames = mat['filenames']\n","        row = np.argwhere(filenames == entry['img_name'])[0][0]\n","        side = entry['side']\n","\n","        img = mat['data'][side][0, 0]['image'][0, 0][row]\n","\n","        if side == 'right':\n","            img = np.fliplr(img)\n","        \n","        img = self.trans(img)\n","\n","        (x, y, z) = mat['data'][side][0, 0]['gaze'][0, 0][row]\n","\n","        theta = np.arcsin(-y)\n","        phi = np.arctan2(-x, -z)\n","        gaze = np.array([-theta, phi])\n","\n","        return {\n","            'img': img,\n","            'gaze': gaze,\n","            'side': side,\n","            'name' : mat_path + entry['img_name'] + entry['side']\n","        }"]},{"cell_type":"code","source":["def pitchyaw_to_vector(pitchyaws):\n","    r\"\"\"Convert given yaw (:math:`\\theta`) and pitch (:math:`\\phi`) angles to unit gaze vectors.\n","    Args:\n","        pitchyaws (:obj:`numpy.array`): yaw and pitch angles :math:`(n\\times 2)` in radians.\n","    Returns:\n","        :obj:`numpy.array` of shape :math:`(n\\times 3)` with 3D vectors per row.\n","    \"\"\"\n","    n = pitchyaws.shape[0]\n","    sin = np.sin(pitchyaws)\n","    cos = np.cos(pitchyaws)\n","    out = np.empty((n, 3))\n","    out[:, 0] = np.multiply(cos[:, 0], sin[:, 1])\n","    out[:, 1] = sin[:, 0]\n","    out[:, 2] = np.multiply(cos[:, 0], cos[:, 1])\n","    return out"],"metadata":{"id":"mfiHqdbwKB05"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["radians_to_degrees = 180.0 / np.pi\n","\n","\n","def angularError(a, b):\n","    \"\"\"Calculate angular error (via cosine similarity).\"\"\"\n","    a = pitchyaw_to_vector(a) if a.shape[1] == 2 else a\n","    b = pitchyaw_to_vector(b) if b.shape[1] == 2 else b\n","\n","    ab = np.sum(np.multiply(a, b), axis=1)\n","    a_norm = np.linalg.norm(a, axis=1)\n","    b_norm = np.linalg.norm(b, axis=1)\n","\n","    # Avoid zero-values (to avoid NaNs)\n","    a_norm = np.clip(a_norm, a_min=1e-7, a_max=None)\n","    b_norm = np.clip(b_norm, a_min=1e-7, a_max=None)\n","\n","    similarity = np.divide(ab, np.multiply(a_norm, b_norm))\n","\n","    return np.arccos(similarity) * radians_to_degrees"],"metadata":{"id":"ctHiKiq1KE_n"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["datasetm = MPIIGaze()\n","\n","print('N', len(datasetm))\n","for i, sample in enumerate(datasetm):\n","  print(sample['name'])\n","  img = sample['img'][None].to(device)\n","  break\n","\n","plt.imshow(img[0][0])"],"metadata":{"id":"DVOR6o13KFc_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def pitchyaw_to_vector(pitchyaws):\n","    r\"\"\"Convert given yaw (:math:`\\theta`) and pitch (:math:`\\phi`) angles to unit gaze vectors.\n","    Args:\n","        pitchyaws (:obj:`numpy.array`): yaw and pitch angles :math:`(n\\times 2)` in radians.\n","    Returns:\n","        :obj:`numpy.array` of shape :math:`(n\\times 3)` with 3D vectors per row.\n","    \"\"\"\n","    n = pitchyaws.shape[0]\n","    sin = torch.sin(pitchyaws)\n","    cos = torch.cos(pitchyaws)\n","    out = torch.empty((n, 3))\n","    out[:, 0] = torch.mul(cos[:, 0], sin[:, 1])\n","    out[:, 1] = sin[:, 0]\n","    out[:, 2] = torch.mul(cos[:, 0], cos[:, 1])\n","    return out\n","\n","radians_to_degrees = 180.0 / np.pi\n","\n","\n","def angularError(a, b):\n","    \"\"\"Calculate angular error (via cosine similarity).\"\"\"\n","    a = pitchyaw_to_vector(a) if a.shape[1] == 2 else a\n","    b = pitchyaw_to_vector(b) if b.shape[1] == 2 else b\n","\n","    ab = torch.sum(torch.mul(a, b), dim=1)\n","    a_norm = torch.linalg.norm(a, dim=1)\n","    b_norm = torch.linalg.norm(b, dim=1)\n","\n","    # Avoid zero-values (to avoid NaNs)\n","    a_norm = torch.clamp(a_norm, min=1e-7,)\n","    b_norm = torch.clamp(b_norm, min=1e-7,)\n","\n","    similarity = torch.div(ab, torch.mul(a_norm, b_norm))\n","\n","    return torch.acos(similarity) * radians_to_degrees"],"metadata":{"id":"z9KTVoH3KJIQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["load_name = \"/content/drive/MyDrive/eye_w/weights/trans_2_3e4_att_256_1learnparam_noNorm_land_alt_MdataN-Step-Checkpoint_30_62160.ckpt\"\n","\n","proj_a = Gaze_Track_pl.load_from_checkpoint(load_name)\n","\n","proj_a.freeze()\n","proj_a.eval()\n","#proj_a.to('cuda')"],"metadata":{"id":"j9UuQx7LKxLq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#from util.preprocess import gaussian_2d\n","#from matplotlib import pyplot as plt\n","#import util.gaze\n","\n","#from eye.gaze_track.utils import draw_gaze\n","#import matplotlib.pyplot as plt\n","\n","\n","datasetm = MPIIGaze()\n","\n","eyenet = proj_a.to(device)\n","\n","errors = []\n","\n","biggest_errors = [0] * 20\n","index_of_big_error = [0] * 20\n","\n","random_indexes = sorted(\n","    np.random.choice(len(datasetm), size=20, replace=False)\n",")\n","\n","print('N', len(datasetm))\n","for i, sample in enumerate(datasetm):\n","  img = sample['img'][None].to(device)\n","\n","  gaze_pred2, heatmaps_pred2, landmarks_pred2 = eyenet.forward(img)\n","\n","  gaze2 = torch.from_numpy(sample['gaze'].reshape((1, 2)))\n","  #gaze_pred2 = np.asarray(gaze_pred2.cpu().numpy())\n","\n","  if sample['side'] == 'right':\n","      gaze_pred2[:, 1] = -gaze_pred2[:, 1]\n","\n","  angular_error = angularError(gaze2, gaze_pred2)\n","\n","  for j, errorval in enumerate(biggest_errors):\n","    if angular_error > errorval:\n","      biggest_errors[j] = angular_error\n","\n","      sorted_indexes = sorted(range(len(biggest_errors)),key=biggest_errors.__getitem__)\n","      biggest_errors = sorted(biggest_errors)\n","\n","      index_of_big_error[sorted_indexes.index(j)] = i\n","\n","      break\n"," \n","\n","  errors.append(angular_error)\n","\n","  \"\"\"\n","  gaze_pred2 = gaze_pred2[0]\n","\n","\n","  landmarks_pred2 = proj_a.tranform_into_actual_coor(landmarks_pred2)\n","  landmarks_pred2 = landmarks_pred2[0].numpy()#.detach().numpy()\n","\n","\n","  eye_img = np.copy(img[0])\n","  eye_img = eye_img.transpose(1, 2, 0)\n","  eye_img = cv2.merge((eye_img,eye_img,eye_img))\n","\n","  eye_img = cv2.normalize(eye_img, None, alpha = 0, beta = 255, norm_type = cv2.NORM_MINMAX, dtype = cv2.CV_32F)\n","  eye_img = eye_img.astype(np.uint8)\n","\n","  landmarks_pred2[:, [1, 0]] = landmarks_pred2[:, [0, 1]]\n","\n","  for (x, y) in landmarks_pred2[0:]:\n","      eye_img = cv2.circle(eye_img, (int(x), int(y)), 1, color=(0, 255, 0), thickness=3)\n","    \n","  eye_img = draw_gaze(eye_img, landmarks_pred2[-1], gaze_pred2, color=(255, 0, 0))\n","  \n","  plt.imshow(eye_img)\n","  \"\"\"\n","\n","  if i%1000 == 0:\n","    print('---', i)\n","    print('error', angular_error)\n","    print('mean error', torch.mean(torch.stack(errors)))\n","    print('side', sample['side'])\n","    print('gaze', gaze2)\n","    print('gaze pred', gaze_pred2)\n"],"metadata":{"id":"_sGIDwX2KJh2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print('mean error', torch.mean(torch.stack(errors)))"],"metadata":{"id":"Oplz_hwyKJp5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["w = 25\n","h = 25\n","fig = plt.figure(figsize=(24, 24))\n","columns = 5\n","rows = 4\n","\n","i = 0\n","\n","for index in random_indexes:\n","\n","  img = dataset[index]['img'][None]#.to(device)\n","  gaze_pred, heatmaps_pred, landmarks_pred = eyenet.forward(img)\n","  gaze = dataset[index]['gaze'].reshape((1, 2))\n","  gaze_pred = np.asarray(gaze_pred.numpy()) # .cpu()  \n","\n","  if sample['side'] == 'right':\n","    print(\"right\")\n","    gaze_pred[0, 1] = -gaze_pred[0, 1]\n","  else:\n","    print(\"left\")\n","\n","  angular_error = angularError(gaze, gaze_pred)\n","  \n","  gaze_pred = gaze_pred[0]\n","  gaze = gaze[0]\n","\n","  landmarks_pred = landmarks_pred.numpy()\n","\n","  landmarks_pred = proj_a.tranform_into_actual_coor(landmarks_pred)\n","  landmarks_pred = landmarks_pred[0]\n","\n","  eye_img = np.copy(img[0])\n","  eye_img = eye_img.transpose(1, 2, 0)\n","  eye_img = cv2.merge((eye_img,eye_img,eye_img))\n","\n","  eye_img = cv2.normalize(eye_img, None, alpha = 0, beta = 255, norm_type = cv2.NORM_MINMAX, dtype = cv2.CV_32F)\n","  eye_img = eye_img.astype(np.uint8)\n","\n","  landmarks_pred[:, [1, 0]] = landmarks_pred[:, [0, 1]]\n","\n","  for (x, y) in landmarks_pred[0:]:\n","    eye_img = cv2.circle(eye_img, (int(x), int(y)), 1, color=(0, 255, 0), thickness=3)\n","    \n","  eye_img = draw_gaze(eye_img, landmarks_pred[-1], gaze_pred, color=(255, 0, 0))\n","\n","  eye_img = draw_gaze(eye_img, landmarks_pred[-1], gaze, color=(0, 255, 0))\n","\n","  print(index, angular_error)\n","\n","  fig.add_subplot(rows, columns, i+1)\n","  plt.imshow(eye_img)\n","\n","  i+=1\n","\n","\n","plt.show()"],"metadata":{"id":"1KkxhQPrKQwz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#\n","w = 25\n","h = 25\n","fig = plt.figure(figsize=(20, 20))\n","columns = 5\n","rows = 2\n","for i in range(0, columns*rows):\n","\n","    print(\"before norm\")\n","    print(i, xx['img'][i].size(), torch.min(xx['img'][i]), torch.max(xx['img'][i]))\n","    \n","    xx['img'][i] = dd(xx['img'][i][None].to('cuda'), 1)\n","    xx['img'][i] = xx['img'][i].to('cpu')[0]\n","\n","    print(\"after norm\")\n","    print(i, xx['img'][i].size(), torch.min(xx['img'][i]), torch.max(xx['img'][i]))\n","\n","    gaze = xx['gaze'][i].detach().numpy()\n","    image = xx['img'][i].detach().numpy()\n","    landmarks = np.copy(xx['landmarks'][i].detach().numpy())\n","\n","    eye_img = np.copy(image)\n","    eye_img = eye_img.transpose(1, 2, 0)\n","    eye_img = cv2.merge((eye_img,eye_img,eye_img))\n","\n","    eye_img = cv2.normalize(eye_img, None, alpha = 0, beta = 255, norm_type = cv2.NORM_MINMAX, dtype = cv2.CV_32F)\n","    eye_img = eye_img.astype(np.uint8)\n","\n","    landmarks[:, [1, 0]] = landmarks[:, [0, 1]]\n","\n","    for (x, y) in landmarks[0:]:\n","        eye_img = cv2.circle(eye_img, (int(x), int(y)), 1, color=(0, 255, 0), thickness=3)\n","        \n","    eye_img = draw_gaze(eye_img, landmarks[-1], gaze, color=(255, 0, 0))\n","\n","    fig.add_subplot(rows, columns, i+1)\n","    plt.imshow(eye_img)\n","\n","plt.show()"],"metadata":{"id":"YYdIfjxXKkxu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"KVD3KkFILES5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["load_name = \"/content/drive/MyDrive/eye_w/weights/trans_2_3e4_att_256_1learnparam_noNorm_land_alt_MdataN-Step-Checkpoint_30_62160.ckpt\"\n","\n","proj_a = Gaze_Track_pl.load_from_checkpoint(load_name)\n","\n","proj_a.freeze()\n","proj_a.eval()\n","#proj_a.to('cuda')"],"metadata":{"id":"CfT7GP4oLEVo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"1-_aM_BCLEu4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["main_path = \"./mpi/MPIIGaze\"\n","\n","path = main_path + '/Data/Normalized'\n","\n","listOfFiles = []\n","for (dirpath, dirnames, filenames) in os.walk(path):\n","    listOfFiles += [os.path.join(dirpath, file) for file in filenames]\n","\n","#full_name = os.path.join(person, day, side, img)"],"metadata":{"id":"C9ujz3_mSGI6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["eval_files = glob.glob(main_path + '/Evaluation Subset/sample list for eye image/*.txt')\n","full_names = read_files_mpi_val(eval_files, path)"],"metadata":{"id":"izI3L4b7SGnt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["images, gazes, images_index_name = process_mpi_files(listOfFiles)"],"metadata":{"id":"2JuXubm4SZ1I"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["list(images_index_name.keys())[0]"],"metadata":{"id":"x97VERXoSbpP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["full_names[0]"],"metadata":{"id":"alz7UcfASdDS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["images_train, gazes_train, images_val, gazes_val = devide_val(images, gazes, images_index_name, full_names)"],"metadata":{"id":"IrsaBm_ySe1X"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["images_train.shape, images_val.shape, images.shape"],"metadata":{"id":"0kvYa0NJSgVO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.imshow(images_val[0])"],"metadata":{"id":"xJCW0_HBSh12"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pre_trained_name_file = \"/content/drive/MyDrive/eye_w/weights/trans_2_3e4_att_256_1learnparam_noNorm_land_alt_MdataN-Step-Checkpoint_29_60088.ckpt\"\n","pretrained_model = Gaze_Track_pl.load_from_checkpoint(pre_trained_name_file)\n","\n","for name, param in pretrained_model.network.named_parameters():\n","  if 'zero_class_token' in name:\n","    print(name)\n","    print(param)\n","    print(param.shape)\n","    param[:, 1:].requires_grad = False#.detach()\n","    print(param)"],"metadata":{"id":"1dO2Pub96bF6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["feature_extcractor.zero_class_token\n","feature_extcractor.patch_embedding.resize.weight\n","feature_extcractor.patch_embedding.resize.bias\n","feature_extcractor.positional_embedding.positional_embedding\n","feature_extcractor.encoder.layers.0.att.block.att.in_proj_weight\n","feature_extcractor.encoder.layers.0.att.block.att.in_proj_bias\n","feature_extcractor.encoder.layers.0.att.block.att.out_proj.weight\n","feature_extcractor.encoder.layers.0.att.block.att.out_proj.bias\n","feature_extcractor.encoder.layers.0.layer_norm.weight\n","feature_extcractor.encoder.layers.0.layer_norm.bias\n","feature_extcractor.encoder.layers.0.ff.w_1.weight\n","feature_extcractor.encoder.layers.0.ff.w_1.bias\n","feature_extcractor.encoder.layers.0.ff.w_2.weight\n","feature_extcractor.encoder.layers.0.ff.w_2.bias\n","feature_extcractor.encoder.layers.0.ff.layer_norm.weight\n","feature_extcractor.encoder.layers.0.ff.layer_norm.bias\n","feature_extcractor.encoder.layers.1.att.block.att.in_proj_weight\n","feature_extcractor.encoder.layers.1.att.block.att.in_proj_bias\n","feature_extcractor.encoder.layers.1.att.block.att.out_proj.weight\n","feature_extcractor.encoder.layers.1.att.block.att.out_proj.bias\n","feature_extcractor.encoder.layers.1.layer_norm.weight\n","feature_extcractor.encoder.layers.1.layer_norm.bias\n","feature_extcractor.encoder.layers.1.ff.w_1.weight\n","feature_extcractor.encoder.layers.1.ff.w_1.bias\n","feature_extcractor.encoder.layers.1.ff.w_2.weight\n","feature_extcractor.encoder.layers.1.ff.w_2.bias\n","feature_extcractor.encoder.layers.1.ff.layer_norm.weight\n","feature_extcractor.encoder.layers.1.ff.layer_norm.bias\n","feature_extcractor.add_train_land.w_1.weight\n","feature_extcractor.add_train_land.w_1.bias\n","feature_extcractor.add_train_land.w_2.weight\n","feature_extcractor.add_train_land.w_2.bias\n","feature_extcractor.add_train_land.layer_norm.weight\n","feature_extcractor.add_train_land.layer_norm.bias\n","gaze_mlp.0.weight\n","gaze_mlp.0.bias\n","gaze_mlp.3.weight\n","gaze_mlp.3.bias\n","landmarks_extract.0.weight\n","landmarks_extract.0.bias\n","landmarks_extract.3.weight\n","landmarks_extract.3.bias"],"metadata":{"id":"ZOMNMUSnmiLe"},"execution_count":null,"outputs":[]}]}